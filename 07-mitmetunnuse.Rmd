# Mitme tunnuse koosanalüüs

Mitme tunnuse koosanalüüs hõlmab mitmesuguseid statistilisi meetodeid, mille abil uuritakse, kuidas erinevad tunnused omavahel seostuvad ja üksteist mõjutavad. See analüüs on oluline, kuna reaalsetes olukordades mõjutavad uuritavat tunnust sageli mitmed tegurid korraga. Koosanalüüsi eesmärk on selgitada välja seosed tunnuste vahel, mõista nende mõju uuritavale nähtusele ja teha prognoose. Tunnuste koosanalüüs aitab minimeerida riski, et olulised seosed jäävad kahe silma vahele.

## Lineaarne regressioon

**Lineaarne regressioon** on statistiline meetod, mida kasutatakse kahe või enama muutuja vahelise lineaarse seose modelleerimiseks. See tähendab, et funktsioontunnuse (sõltuva muutuja) väärtuse muutust saab kirjeldada sirge joonega, mis on argumenttunnuse (sõltumatu muutuja) funktsioon. Regressioonanalüüsi eesmärk on leida mudel, mis kirjeldab võimalikult täpselt andmetes esinevat seost, võimaldades teha prognoose ja mõista muutujate omavahelist mõju.

Lineaarne regressioonimudel, kus on üks argumenttunnus (x) ja üks funktsioontunnus (y), avaldub järgmiselt:

$$y = b_0 + b_1x$$


kus:

*   `y` on funktsioontunnus (sõltuv muutuja), mida me püüame prognoosida.
*   `x` on argumenttunnus (sõltumatu muutuja), mille abil me püüame funktsioontunnust prognoosida.
*   `b_0` on vabaliige (y-telje lõikepunkt), mis näitab funktsioontunnuse väärtust, kui argumenttunnuse väärtus on null.
*   `b_1` on regressioonikordaja (sirge tõus), mis näitab, kui palju muutub funktsioontunnuse väärtus, kui argumenttunnuse väärtus suureneb ühe ühiku võrra.

Mitmese lineaarse regressioonimudel sisaldab mitut argumenttunnust:

$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_kx_k$$


kus:

*   `x_1, x_2, ..., x_k` on erinevad argumenttunnused (regressorid).
*   `b_1, b_2, ..., b_k` on vastavad regressioonikordajad, mis näitavad iga argumenttunnuse mõju funktsioontunnusele, eeldusel, et teiste argumenttunnuste väärtused on konstantsed.
*   `b_0` on vabaliige.

Reaalses elus esineb juhuslikke hälbeid, mistõttu lisatakse mudelisse vealiige $\epsilon$:


$$y = b_0 + b_1x + \epsilon$$

**Parameetrite hindamine ja vähimruutude meetod**

Regressioonimudeli parameetrite (`b_0` ja `b_1`) hindamiseks kasutatakse **vähimruutude meetodit**. Selle meetodi eesmärk on leida parameetrite väärtused, mis minimeerivad funktsioontunnuse tegelike väärtuste (`y_i`) ja mudeli ennustatud väärtuste (`ŷ_i`) vaheliste hälvete ruutude summa (SSE - Sum of Squared Errors):

$$SSE = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

Teisisõnu, vähimruutude meetodil leitakse sirge, mis läbib punktiparve nii, et punktide y-koordinaatide summaarne või keskmine kõrvalekalle regressioonisirgest oleks minimaalne.

**Mudeli sobivuse hindamine**

Pärast parameetrite hindamist on oluline hinnata mudeli sobivust ja usaldusväärsust.

*   **Jääkide analüüs:** Jäägid on tegelike ja mudeli poolt ennustatud väärtuste vahed (`e_i = y_i - \hat{y}_i`). Jääkide analüüs aitab otsustada, kas mudeli kuju on sobiv ja kas esineb ebatüüpilisi objekte ehk erindeid. Jääkide graafiline analüüs, kus jäägid on kantud y-teljele ja x-teljel on kas argumenttunnus või prognoositud väärtused, aitab tuvastada mustreid. Kui jäägid jaotuvad juhuslikult ümber nulli, on mudel sobiv. Kui jääkides on aga märgatav muster (nt kõverjoonelisus), siis ei ole lineaarne mudel sobiv.
*   **Determinatsioonikordaja ($R^2$):** $R^2$ näitab, kui suure osa funktsioontunnuse koguhajuvusest (SST) moodustab regressioonmudeliga kirjeldatud hajuvus (SSR). $R^2$ väärtus jääb vahemikku 0 kuni 1, kus suurem $R^2$ väärtus viitab paremale mudeli sobivusele.

$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

kus:

*   `SST` (Total Sum of Squares) on koguhajuvus:
    
    $SST = \sum_{i=1}^{n}(y_i - \bar{y})^2$
    
*   `SSR` (Sum of Squares due to Regression) on regressioonhajuvus:
    
    $SSR = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$
    
*   `SSE` (Sum of Squares Error) on jääkhajuvus:
    
    $SSE = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$
    
*   $\bar{y}$ on funktsioontunnuse keskväärtus.
*   **Korrigeeritud determinatsioonikordaja ($R^2m$):** $R^2$ väärtus suureneb alati, kui mudelisse lisada uusi tunnuseid, isegi kui need ei ole olulised. $R^2m$ arvestab mudeli keerukust (argumenttunnuste arvu), vältides seeläbi ülepaisutatud mudelite eelistamist. $R^2m$ on eriti kasulik mitmese regressiooni korral.

    
$$R^2_a = 1 - (1-R^2) \frac{n-1}{n-k-1}$$
    

kus `n` on valimi suurus ja `k` on argumenttunnuste arv mudelis.

*   **Mudeli standardviga:** Mudeli standardviga (`s_e`) iseloomustab funktsioontunnuse väärtuste hajumist ümber regressioonsirge, sarnaselt standardhälbega ühe tunnuse korral. Mida väiksem on mudeli standardviga, seda täpsemad on prognoosid.

$$s_e = \sqrt{\frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{n-2}} = \sqrt{\frac{SSE}{n-2}}$$

*   **Parameetrite standardvead ja usalduspiirid:** Regressioonikordajate hinnangud on valimipõhised ja seega esineb neis alati teatud määramatus. Hinnangute täpsust saab hinnata standardvigade ja usalduspiiride abil. Regressioonsirge parameetrite hinnangud alluvad t-jaotusele, mis võimaldab leida vastavad usalduspiirid.
*   **Mudeli olulisuse kontroll:** Kontrollitakse, kas mudel tervikuna on statistiliselt oluline ehk kas seos argument- ja funktsioontunnuse vahel on statistiliselt oluline. Kui mudel ei ole oluline, siis pole mõtet seda edasi uurida.

**Lineaarse regressiooni eeldused**

Lineaarse regressiooni rakendamisel on vaja kontrollida järgmisi eeldusi:

*   **Lineaarsus:** Eeldatakse, et seos funktsioontunnuse ja argumenttunnuste vahel on lineaarne. Kui seos ei ole lineaarne, tuleks kaaluda mittelineaarset regressiooni.
*   **Sõltumatus:** Iga vaatlus peab olema sõltumatu teistest vaatlustest.
*   **homoskedastilisus:** Jääkide dispersioon peaks olema konstantne kõigi argumenttunnuste väärtuste korral (jääkide hajuvus ei tohiks süstemaatiliselt muutuda).
*   **Normaaljaotus:** Jäägid peaksid olema ligikaudselt normaaljaotusega.
*   **Multikollineaarsuse puudumine:** Argumenttunnused ei tohiks omavahel tugevalt korreleeruda, sest see raskendab üksikute tunnuste mõju eristamist ja teeb regressioonikordajate hinnangud ebastabiilseks.

**Lineaarse mudeli variatsioonid**

*   **Regressioon läbi nullpunkti:** Mõnikord on vaja hinnata lineaarset mudelit, kus vabaliige on null (`b_0 = 0`). See tähendab, et regressioonsirge läbib koordinaatide alguspunkti. Sellisel juhul ei saa kasutada tavapärast determinatsioonikordajat `R²`.
*   **Standardiseeritud kordajad:** Standardiseeritud kordajad näitavad, mitme standardhälbe võrra muutub funktsioontunnus, kui argumenttunnus suureneb ühe standardhälbe võrra. Standardiseeritud tunnustega lineaarse mudeli vabaliige on 0, st regressioonijoon läbib nullpunkti.
*   **Logaritmiline mudel:** Mittelineaarsete seoste modelleerimiseks on mõnikord kasulik logaritmida nii funktsioontunnust kui ka argumenttunnuseid. See võimaldab teatud juhtudel teisendada mittelineaarse seose lineaarseks ja kasutada seeläbi lineaarset mudelit.

